{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, Normalizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loading boston dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_dataset = load_boston()\n",
    "X = boston_dataset.data\n",
    "y = boston_dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.reshape(y, (-1, 1))\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**splitting boston dataset into training and testing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenth of training data = 379\n",
      "length of testing data = 127\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/4, random_state=0)\n",
    "print(f'lenth of training data = {len(X_train)}\\nlength of testing data = {len(X_test)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**use of lable encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial array = ['Female' 'Female' 'Male' 'Male' 'Female' 'Male' 'Female' 'Male' 'Female'\n",
      " 'Female']\n",
      "Encoded array = [0 0 1 1 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#we will create one numpy arry and perform encoding on it\n",
    "data1 = np.random.choice(('Male', 'Female'), size=10)\n",
    "le = LabelEncoder()\n",
    "data1_encoded = le.fit_transform(data1)\n",
    "print(f'Initial array = {data1}')\n",
    "print(f'Encoded array = {data1_encoded}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**handling NaN's using SimpleImputer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data :\n",
      " [[ 1. nan  2.]\n",
      " [ 2.  3. nan]\n",
      " [-1.  4.  2.]]\n",
      "\n",
      "Strategy = Mean :\n",
      " [[ 1.   3.5  2. ]\n",
      " [ 2.   3.   2. ]\n",
      " [-1.   4.   2. ]]\n",
      "\n",
      " Strategy = Median :\n",
      " [[ 1.   3.5  2. ]\n",
      " [ 2.   3.   2. ]\n",
      " [-1.   4.   2. ]]\n",
      "\n",
      " Strategy = Most Frequent :\n",
      " [[ 1.  3.  2.]\n",
      " [ 2.  3.  2.]\n",
      " [-1.  4.  2.]]\n"
     ]
    }
   ],
   "source": [
    "#we will create 2d numpy array with NaN values in it and then handle\n",
    "#them with SimpleImputer using three strategies\n",
    "data2 = np.array(\n",
    "    [\n",
    "        [1,np.nan,2],\n",
    "        [2,3,np.nan],\n",
    "        [-1,4,2]\n",
    "    ]\n",
    ")\n",
    "mean_imp = SimpleImputer(strategy='mean')\n",
    "median_imp = SimpleImputer(strategy='median')\n",
    "most_freq_imp = SimpleImputer(strategy='most_frequent')\n",
    "print(\n",
    "    f'Original data :\\n {data2}\\n\\n'\n",
    "    f'Strategy = Mean :\\n {mean_imp.fit_transform(data2)}\\n\\n',\n",
    "    f'Strategy = Median :\\n {median_imp.fit_transform(data2)}\\n\\n',\n",
    "    f'Strategy = Most Frequent :\\n {most_freq_imp.fit_transform(data2)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using Normalizer to normalize values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "originl data :\n",
      " [[1.0, -1.0, 2.0], [2.0, 0.0, 0.0], [0.0, 1.0, -1.0]]\n",
      "\n",
      " max normalization :\n",
      " [[ 0.5 -0.5  1. ]\n",
      " [ 1.   0.   0. ]\n",
      " [ 0.   1.  -1. ]]\n",
      "\n",
      " l1 normalization :\n",
      " [[ 0.25 -0.25  0.5 ]\n",
      " [ 1.    0.    0.  ]\n",
      " [ 0.    0.5  -0.5 ]]\n",
      "\n",
      " l2 normalization :\n",
      " [[ 0.40824829 -0.40824829  0.81649658]\n",
      " [ 1.          0.          0.        ]\n",
      " [ 0.          0.70710678 -0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "#we will create 2d array and perform 3 types of normaisations on it\n",
    "data3 =[\n",
    "        [ 1., -1.,  2.],\n",
    "        [ 2.,  0.,  0.],\n",
    "        [ 0.,  1., -1.]\n",
    "]\n",
    "norm_max = Normalizer(norm='max')\n",
    "norm_l1 = Normalizer(norm='l1')\n",
    "norm_l2 = Normalizer(norm='l2')\n",
    "print(\n",
    "    f'originl data :\\n {data3}\\n\\n',\n",
    "    f'max normalization :\\n {norm_max.fit_transform(data3)}\\n\\n',\n",
    "    f'l1 normalization :\\n {norm_l1.fit_transform(data3)}\\n\\n',\n",
    "    f'l2 normalization :\\n {norm_l2.fit_transform(data3)}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using variance thereshold to neglet fetures with less variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data (1st five rows for representation) :\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
      "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
      "  4.0300e+00]]\n",
      "\n",
      " Number of features : 13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#we will take boston data that was imported in 2nd cell\n",
    "#and we will remove the features that has very less variance\n",
    "print(\n",
    "    f'Data (1st five rows for representation) :\\n{X[:3, :]}\\n\\n',\n",
    "    f'Number of features : {len(X[0])}\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after removing features having variance 1.5 :\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 6.5200e+01 4.0900e+00 1.0000e+00\n",
      "  2.9600e+02 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 7.8900e+01 4.9671e+00 2.0000e+00\n",
      "  2.4200e+02 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 6.1100e+01 4.9671e+00 2.0000e+00\n",
      "  2.4200e+02 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " [3.2370e-02 0.0000e+00 2.1800e+00 4.5800e+01 6.0622e+00 3.0000e+00\n",
      "  2.2200e+02 1.8700e+01 3.9463e+02 2.9400e+00]\n",
      " [6.9050e-02 0.0000e+00 2.1800e+00 5.4200e+01 6.0622e+00 3.0000e+00\n",
      "  2.2200e+02 1.8700e+01 3.9690e+02 5.3300e+00]]\n",
      "\n",
      " New number of features : 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "varience_threshold = VarianceThreshold(threshold=1.5)\n",
    "X_after_applying_threshold = varience_threshold.fit_transform(X)\n",
    "print(\n",
    "    f'Data after removing features having variance 1.5 :\\n{X_after_applying_threshold[:5, :]}\\n\\n',\n",
    "    f'New number of features : {len(X_after_applying_threshold[0])}\\n'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
